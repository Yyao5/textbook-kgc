<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2299.4">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px Times}
    span.s1 {color: #000066}
    span.s2 {font: 10.0px Helvetica}
    span.s3 {font: 7.5px Times}
    span.s4 {font: 7.5px Times; color: #000066}
  </style>
</head>
<body>
<p class="p1">The technological capacity to store information has grown over time at an accelerated pace [<span class="s1">164</span>,178]:</p>
<p class="p1">• 1986: 2.6 EB; equivalent to less than one 730 MB CD-ROM of data per computer user.</p>
<p class="p1">• 1993: 15.8 EB; equivalent to four CD-ROMs per user.</p>
<p class="p1">• 2000: 54.5 EB; equivalent to 12 CD-ROMs per user.</p>
<p class="p1">• 2007: 295 EB; equivalent to almost 61 CD-ROMs per user.</p>
<p class="p1">Though it pales in comparison with processor technology, the evolution of storage technology is</p>
<p class="p1">astounding. A 2003 study [<span class="s1">251</span>] shows that during the 1980–2003 period the storage density of hard</p>
<p class="p1">disk drives (HDD) increased by four orders of magnitude, from about 0<span class="s2">.</span>01 Gb/in<span class="s3">2 </span>to about 100 Gb/in<span class="s3">2</span>.</p>
<p class="p1">During the same period the prices fell by five orders ofmagnitude, to about 1 cent/Mbyte.HDDdensities</p>
<p class="p1">are projected to climb to 1<span class="s2">,</span>800 Gb/in<span class="s3">2 </span>by 2016, up from 744 Gb/in<span class="s3">2 </span>in 2011.</p>
<p class="p1">The density of Dynamic Random Access Memory (DRAM ) increased from about 1 Gb/in<span class="s3">2 </span>in 1990</p>
<p class="p1">to 100 Gb/in<span class="s3">2 </span>in 2003. The cost of DRAM tumbled from about $80<span class="s2">/</span>MB to less than $1<span class="s2">/</span>MB during the</p>
<p class="p1">same period. In 2010 Samsung announced the first monolithic, 4 gigabit, low-power, double-data-rate</p>
<p class="p1">(LPDDR2) DRAM using a 30 nm process.</p>
<p class="p1">These rapid technological advancements have changed the balance between initial investment in</p>
<p class="p1">storage devices and system management costs. Now the cost of storage management is the dominant</p>
<p class="p1">element of the total cost of a storage system. This effect favors the centralized storage strategy supported</p>
<p class="p1">by a cloud; indeed, a centralized approach can automate some of the storage management functions,</p>
<p class="p1">such as replication and backup, and thus reduce substantially the storage management cost.</p>
<p class="p1">While the density of storage devices has increased and the cost has decreased dramatically, the</p>
<p class="p1">access time has improved only slightly. The performance of I/O subsystems has not kept pace with the</p>
<p class="p1">performance of computing engines, and that affects multimedia, scientific and engineering, and other</p>
<p class="p1">modern applications that process increasingly large volumes of data.</p>
<p class="p1">The storage systems face substantial pressure because the volume of data generated has increased</p>
<p class="p1">exponentially during the past few decades; whereas in the 1980s and 1990s data was primarily generated</p>
<p class="p1">by humans, nowadays machines generate data at an unprecedented rate. Mobile devices, such</p>
<p class="p1">as smart-phones and tablets, record static images, as well as movies and have limited local storage</p>
<p class="p1">capacity, so they transfer the data to cloud storage systems. Sensors, surveillance cameras, and digital</p>
<p class="p1">medical imaging devices generate data at a high rate and dump it onto storage systems accessible via</p>
<p class="p1">the Internet. Online digital libraries, ebooks, and digital media, along with reference data,<span class="s4">2 </span>add to the</p>
<p class="p1">demand for massive amounts of storage.</p>
<p class="p1">As the volume of data increases, new methods and algorithms for data mining that require powerful</p>
<p class="p1">computing systems have been developed. Only a concentration of resources could provide the CPU cycles along with the vast storage capacity necessary to perform such intensive computations and</p>
<p class="p1">access the very large volume of data.</p>
<p class="p1">Although we emphasize the advantages of a concentration of resources, we have to be acutely aware</p>
<p class="p1">that a cloud is a large-scale distributed system with a very large number of components that must</p>
<p class="p1">work in concert. The management of such a large collection of systems poses significant challenges</p>
<p class="p1">and requires novel approaches to systems design. Case in point: Although the early distributed file</p>
<p class="p1">systems used custom-designed reliable components, nowadays large-scale systems are built with offthe-</p>
<p class="p1">shelf components. The emphasis of the design philosophy has shifted from <span class="s2">performance at any</span></p>
<p class="p1"><span class="s2">cost </span>to <span class="s2">reliability at the lowest possible cost</span>. This shift is evident in the evolution of ideas, from the</p>
<p class="p1">early distributed file systems of the 1980s, such as the Network File System (NFS) and the Andrew File</p>
<p class="p1">System (AFS), to today’s Google File System (GFS) and the <span class="s2">Megastore</span>.</p>
</body>
</html>
