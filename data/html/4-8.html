<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2299.4">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px Times}
    span.s1 {font: 10.0px Helvetica}
    span.s2 {color: #000066}
    span.s3 {font: 7.5px Times; color: #000066}
  </style>
</head>
<body>
<p class="p1">For more than two thousand years of human history, science was empirical. Several hundred years ago</p>
<p class="p1">theoretical methods based on models and generalization were introduced, allowing substantial progress</p>
<p class="p1">in human knowledge. In the last few decades, we have witnessed the explosion of computational science</p>
<p class="p1">based on the simulation of complex phenomena.</p>
<p class="p1">In a talk delivered in 2007 and posted on his Web site just before he went missing in January</p>
<p class="p1">2007, computer scientist Jim Gray discussed <span class="s1">eScience </span>as a transformative scientific method [<span class="s2">163</span>].</p>
<p class="p1">Today, <span class="s1">eScience </span>unifies experiment, theory, and simulation; data captured from measuring instruments</p>
<p class="p1">or generated by simulations are processed by software systems, and data and knowledge are stored by</p>
<p class="p1">computer systems and analyzed using statistical packages.</p>
<p class="p1">The generic problems in virtually all areas of science are:</p>
<p class="p1">• Collecting experimental data.</p>
<p class="p1">• Managing very large volumes of data.</p>
<p class="p1">• Building and executing models.</p>
<p class="p1">• Integrating data and literature.</p>
<p class="p1">• Documenting experiments.</p>
<p class="p1">• Sharing the data with others; data preservation for long periods of time.</p>
<p class="p1">All these activities require powerful computing systems.</p>
<p class="p1">A typical example of a problem faced by agencies and research groups is data discovery in large</p>
<p class="p1">scientific data sets. Examples of such large collections are the biomedical and genomic data at NCBI,<span class="s3">8</span></p>
<p class="p1">the astrophysics data at NASA,<span class="s3">9 </span>or the atmospheric data at NOAA<span class="s3">10 </span>and NCAR.<span class="s3">11</span></p>
<p class="p1">The process of online data discovery can be viewed as an ensemble of several phases [<span class="s2">282</span>]: (i)</p>
<p class="p1">recognition of the information problem; (ii) generation of search queries using one or more search</p>
<p class="p1">engines; (iii) evaluation of the search results; (iv) evaluation of theWeb documents; and (v) comparison</p>
<p class="p1">of information from different sources. The Web search technology allows scientists to discover text</p>
<p class="p1">documents related to such data, but the binary encoding of many of the documents poses serious</p>
<p class="p1">challenges.</p>
<p class="p1">Metadata is used to describe digital data and provides an invaluable aid for discovering useful</p>
<p class="p1">information in a scientific data set. A recent paper [<span class="s2">282</span>] describes a system for data discovery that</p>
<p class="p1">supports automated fine-grained metadata extraction and summarization schemes for browsing large</p>
<p class="p1">data sets and is extensible to different scientific domains. The system, called <span class="s1">Glean</span>, is designed to run</p>
<p class="p1">on a computer cluster or on a cloud; its run-time system supports two computational models, one based</p>
<p class="p1">on <span class="s1">MapReduce </span>and the other on graph-based orchestration.</p>
</body>
</html>
