<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2299.4">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px Times}
    span.s1 {font: 7.5px Times; color: #000066}
    span.s2 {font: 10.0px Helvetica}
    span.s3 {font: 10.0px Courier}
  </style>
</head>
<body>
<p class="p1">The concepts and technologies for network-centric computing and content evolved through the years</p>
<p class="p1">and led to several large-scale distributed system developments:</p>
<p class="p1">• The Web and the semantic Web are expected to support composition of services (not necessarily</p>
<p class="p1">computational services) available on the Web.<span class="s1">1</span></p>
<p class="p1">• The Grid, initiated in the early 1990s by National Laboratories and Universities, is used primarily</p>
<p class="p1">for applications in the area of science and engineering.</p>
<p class="p1">• Computer clouds, promoted since 2005 as a form of service-oriented computing by large IT companies,</p>
<p class="p1">are used for enterprise computing, high-performance computing, Web hosting, and storage</p>
<p class="p1">for network-centric content.</p>
<p class="p1">The need to share data from high-energy physics experiments motivated Sir Tim Berners-Lee, who</p>
<p class="p1">worked at the European Organization for Nuclear Research (CERN) in the late 1980s, to put together</p>
<p class="p1">the two major components of the World Wide Web: HyperText Markup Language (HTML) for data</p>
<p class="p1">description and HyperText Transfer Protocol (HTTP) for data transfer. The Web opened a new era in</p>
<p class="p1">data sharing and ultimately led to the concept of network-centric content.</p>
<p class="p1">The semantic Web<span class="s1">2 </span>is an effort to enable laypeople to more easily find, share, and combine information</p>
<p class="p1">available on the Web. In this vision, the information can be readily interpreted by machines,</p>
<p class="p1">so machines can perform more of the tedious work involved in finding, combining, and acting upon</p>
<p class="p1">information on theWeb. Several technologies are necessary to provide a formal description of concepts,</p>
<p class="p1">terms, and relationships within a given knowledge domain; they include the Resource Description</p>
<p class="p1">Framework (RDF), a variety of data interchange formats, and notations such as RDF Schema (RDFS)</p>
<p class="p1">and the Web Ontology Language (OWL). Gradually, the need to make computing more affordable and to liberate users from the concerns</p>
<p class="p1">regarding system and software maintenance reinforced the idea of concentrating computing resources</p>
<p class="p1">in data centers. Initially, these centers were specialized, each running a limited palette of software</p>
<p class="p1">systems as well as applications developed by the users of these systems. In the early 1980s major</p>
<p class="p1">research organizations such as the National Laboratories and large companies had powerful computing</p>
<p class="p1">centers supporting large user populations scattered throughout wide geographic areas. Then the idea</p>
<p class="p1">to link such centers in an infrastructure resembling the power grid was born; the model known as</p>
<p class="p1">network-centric computing was taking shape.</p>
<p class="p1">A <span class="s2">computing grid </span>is a distributed system consisting of a large number of loosely coupled, heterogeneous,</p>
<p class="p1">and geographically dispersed systems in different administrative domains. The term <span class="s2">computing</span></p>
<p class="p1"><span class="s2">grid </span>is a metaphor for accessing computer power with similar ease as we access power provided by the</p>
<p class="p1">electric grid. Software libraries known as <span class="s2">middleware </span>have been furiously developed since the early</p>
<p class="p1">1990s to facilitate access to grid services.</p>
<p class="p1">The vision of the grid movement was to give a user the illusion of a very large virtual supercomputer.</p>
<p class="p1">The autonomy of the individual systems and the fact that these systems were connected by wide-area</p>
<p class="p1">networks with latency higher than the latency of the interconnection network of a supercomputer posed</p>
<p class="p1">serious challenges to this vision. Nevertheless, several “Grand Challenge” problems, such as protein</p>
<p class="p1">folding, financial modeling, earthquake simulation, and climate and weather modeling, run successfully</p>
<p class="p1">on specialized grids. The Enabling Grids for Escience project is arguably the largest computing grid;</p>
<p class="p1">along with the LHC Computing Grid (LCG), the Escience project aims to support the experiments using</p>
<p class="p1">the Large Hadron Collider (LHC) at CERN which generate several gigabytes of data per second, or</p>
<p class="p1">10 PB (petabytes) per year.</p>
<p class="p1">In retrospect, two basic assumptions about the infrastructure prevented the grid movement from</p>
<p class="p1">having the impact its supporters were hoping for. The first is the heterogeneity of the individual systems</p>
<p class="p1">interconnected by the grid; the second is that systems in different administrative domains are expected to</p>
<p class="p1">cooperate seamlessly. Indeed, the heterogeneity of the hardware and of system software poses significant</p>
<p class="p1">challenges for application development and for application mobility. At the same time, critical areas</p>
<p class="p1">of system management, including scheduling, optimization of resource allocation, load balancing, and</p>
<p class="p1">fault tolerance, are extremely difficult in a heterogeneous system. The fact that resources are in different</p>
<p class="p1">administrative domains further complicates many already difficult problems related to security and</p>
<p class="p1">resource management. Although very popular in the science and engineering communities, the grid</p>
<p class="p1">movement did not address the major concerns of the enterprise computing communities and did not</p>
<p class="p1">make a noticeable impact on the IT industry.</p>
<p class="p1">Cloud computing is a technology largely viewed as the next big step in the development and deployment</p>
<p class="p1">of an increasing number of distributed applications. The companies promoting cloud computing</p>
<p class="p1">seem to have learned themost important lessons from the gridmovement. Computer clouds are typically</p>
<p class="p1">homogeneous. An entire cloud shares the same security, resource management, cost and other policies,</p>
<p class="p1">and last but not least, it targets enterprise computing. These are some of the reasons that several agencies</p>
<p class="p1">of the US Government, including Health and Human Services (HHS), the Centers for Disease Control</p>
<p class="p1">(CDC), the National Aeronautics and Space Administration (NASA), the Navy’s Next Generation</p>
<p class="p1">Enterprise Network (NGEN), and the Defense Information Systems Agency (DISA), have launched</p>
<p class="p1">cloud computing initiatives and conduct actual system development intended to improve the efficiency</p>
<p class="p1">and effectiveness of their information processing needs.</p>
<p class="p1">The term <span class="s2">content </span>refers to any type or volume of media, be it static or dynamic, monolithic or</p>
<p class="p1">modular, live or stored, produced by aggregation, or mixed. <span class="s2">Information </span>is the result of functions</p>
<p class="p1">applied to content. The creation and consumption of audio and visual content are likely to transform</p>
<p class="p1">the Internet to support increased quality in terms of resolution, frame rate, color depth, and stereoscopic</p>
<p class="p1">information, and it seems reasonable to assume that the Future Internet<span class="s1">3 </span>will be content-centric. The</p>
<p class="p1">content should be treated as having meaningful semantic connotations rather than a string of bytes;</p>
<p class="p1">the focus will be the information that can be extracted by content mining when users request named</p>
<p class="p1">data and content providers publish data objects. Content-centric routing will allow users to fetch the</p>
<p class="p1">desired data from the most suitable location in terms of network latency or download time. There are</p>
<p class="p1">also some challenges, such as providing secure services for content manipulation, ensuring global rights</p>
<p class="p1">management, control over unsuitable content, and reputation management.</p>
<p class="p1">Network-centric computing and network-centric content share a number of characteristics:</p>
<p class="p1">• Most applications are data-intensive. Computer simulation becomes a powerful tool for scientific</p>
<p class="p1">research in virtually all areas of science, from physics, biology, and chemistry to archeology. Sophisticated</p>
<p class="p1">tools for computer-aided design, such as Catia (Computer Aided Three-dimensional Interactive</p>
<p class="p1">Application), are widely used in the aerospace and automotive industries. Thewidespread use of sensors</p>
<p class="p1">contributes to increases in the volume of data. Multimedia applications are increasingly popular;</p>
<p class="p1">the ever-larger media increase the load placed on storage, networking, and processing systems.</p>
<p class="p1">• Virtually all applications are network-intensive. Indeed, transferring large volumes of data requires</p>
<p class="p1">high-bandwidth networks; parallel computing, computation steering,<span class="s1">4 </span>and data streaming are examples</p>
<p class="p1">of applications that can only run efficiently on low-latency networks.</p>
<p class="p1">• The systems are accessed using <span class="s2">thin clients </span>running on systems with limited resources. In June 2011</p>
<p class="p1">Google released Google Chrome OS, designed to run on primitive devices and based on the browser</p>
<p class="p1">with the same name.</p>
<p class="p1">• The infrastructure supports some form of workflow management. Indeed, complex computational</p>
<p class="p1">tasks require coordination of several applications; composition of services is a basic tenet ofWeb 2.0.</p>
<p class="p1">The advantages of network-centric computing and network-centric content paradigms are, at the</p>
<p class="p1">same time, sources for concern; we discuss some of them:</p>
<p class="p1">• Computing and communication resources (CPU cycles, storage, network bandwidth) are shared and</p>
<p class="p1">resources can be aggregated to support data-intensive applications. Multiplexing leads to a higher</p>
<p class="p1">resource utilization; indeed, when multiple applications share a system, their peak demands for</p>
<p class="p1">resources are not synchronized and the average system utilization increases. On the other hand,</p>
<p class="p1">the management of large pools of resources poses new challenges as complex systems are subject</p>
<p class="p1">to phase transitions. New resource management strategies, such as self-organization, and decisions</p>
<p class="p1">based on approximate knowledge of the state of the system must be considered. Ensuring quality-ofservice</p>
<p class="p1">(QoS) guarantees is extremely challenging in such environments because total performance</p>
<p class="p1">isolation is elusive.</p>
<p class="p1">• Data sharing facilitates collaborative activities. Indeed, many applications in science, engineering,</p>
<p class="p1">and industrial, financial, and governmental applications require multiple types of analysis of shared</p>
<p class="p1">data sets and multiple decisions carried out by groups scattered around the globe. Open software</p>
<p class="p1">development sites are another example of such collaborative activities. Data sharing poses not only</p>
<p class="p1">security and privacy challenges but also requires mechanisms for access control by authorized users</p>
<p class="p1">and for detailed logs of the history of data changes.</p>
<p class="p1">• Cost reduction. Concentration of resources creates the opportunity to pay as you go for computing</p>
<p class="p1">and thus eliminates the initial investment and reduces significantly the maintenance and operation</p>
<p class="p1">costs of the local computing infrastructure.</p>
<p class="p1">• User convenience and elasticity, that is the ability to accommodate workloads with very large peakto-</p>
<p class="p1">average ratios.</p>
<p class="p1">It is very hard to point out a single technological or architectural development that triggered the</p>
<p class="p1">movement toward network-centric computing and network-centric content. This movement is the result</p>
<p class="p1">of a cumulative effect of developments inmicroprocessor, storage, and networking technologies coupled</p>
<p class="p1">with architectural advancements in all these areas and, last but not least, with advances in software</p>
<p class="p1">systems, tools, programming languages, and algorithms to support distributed and parallel computing.</p>
<p class="p1">Through the years we have witnessed the breathtaking evolution of solid-state technologies which</p>
<p class="p1">led to the development of multicore and many-core processors. Quad-core processors such as the AMD</p>
<p class="p1">Phenom II X4, the Intel i3, i5, and i7 and hexa-core processors such as the AMDPhenom II X6 and Intel</p>
<p class="p1">Core i7 Extreme Edition 980X are now used in the servers populating computer clouds. The proximity</p>
<p class="p1">of multiple cores on the same die allows the cache coherency circuitry to operate at a much higher clock</p>
<p class="p1">rate than would be possible if the signals were to travel off-chip.</p>
<p class="p1">Storage technology has also evolved dramatically. For example, solid-state disks such as RamSan-</p>
<p class="p1">440 allow systems to manage very high transaction volumes and larger numbers of concurrent users.</p>
<p class="p1">RamSan-440 uses DDR2 (double-data-rate) RAM to deliver 600,000 sustained random input/output</p>
<p class="p1">operations per second (IOPS) and over 4 GB/s of sustained random <span class="s3">read </span>or <span class="s3">write </span>bandwidth, with</p>
<p class="p1">latency of less than 15 microseconds, and it is available in 256 GB and 512 GB configurations. The</p>
<p class="p1">price of memory has dropped significantly; at the time of this writing the price of a 1 GB module for a</p>
<p class="p1">PC is approaching $10. Optical storage technologies and Flash memories are widely used nowadays.</p>
<p class="p1">The thinking in software engineering has also evolved and new models have emerged. The <span class="s2">three-tier</span></p>
<p class="p1"><span class="s2">model </span>is a software architecture and a software design pattern. The <span class="s2">presentation tier </span>is the topmost</p>
<p class="p1">level of the application; typically, it runs on a desktop PC or workstation, uses a standard graphical user</p>
<p class="p1">interface (GUI) and displays information related to services such as browsing merchandise, purchasing</p>
<p class="p1">products, and managing shopping cart contents. The presentation tier communicates with other tiers by</p>
<p class="p1">sending the results to the browser/client tier and all other tiers in the network. The <span class="s2">application/logic tier</span></p>
<p class="p1">controls the functionality of an application and may consist of one or more separate modules running</p>
<p class="p1">on a workstation or application server; it may be multitiered itself, in which case the architecture is</p>
<p class="p1">called an <span class="s2">n-tier architecture</span>. The <span class="s2">data tier </span>controls the servers where the information is stored; it runs</p>
<p class="p1">a relational database management system (RDBMS) on a database server or a mainframe and contains</p>
<p class="p1">the computer data storage logic. The data tier keeps data independent from application servers or</p>
<p class="p1">processing logic and improves scalability and performance. Any of the tiers can be replaced independently;</p>
<p class="p1">for example, a change of operating system in the presentation tier would only affect the user</p>
<p class="p1">interface code.</p>
</body>
</html>
