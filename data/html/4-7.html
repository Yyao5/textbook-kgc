<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2299.4">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px Times}
    span.s1 {font: 10.0px Helvetica}
    span.s2 {color: #000066}
    span.s3 {font: 10.0px Courier}
  </style>
</head>
<body>
<p class="p1">An application called <span class="s1">GrepTheWeb</span>, discussed in [<span class="s2">360</span>], is now in production at Amazon. We use it to</p>
<p class="p1">illustrate the power and appeal of cloud computing. The application allows a user to define a regular</p>
<p class="p1">expression and search the Web for records that match it. <span class="s1">GrepTheWeb </span>is analogous to the <span class="s1">Unix grep</span></p>
<p class="p1">command used to search a file for a given regular expression.</p>
<p class="p1">This application performs a search of a very large set of records, attempting to identify records that</p>
<p class="p1">satisfy a regular expression. The source of this search is a collection of document URLs produced by</p>
<p class="p1">the <span class="s1">Alexa Web Search</span>, a software system that crawls theWeb every night. The inputs to the applications</p>
<p class="p1">are a regular expression and the large data set produced by theWeb-crawling software; the output is the</p>
<p class="p1">set of records that satisfy the expression. The user is able to interact with the application and get the</p>
<p class="p1">current status [see Figure <span class="s2">4.7</span>(a)].</p>
<p class="p1">The application usesmessage passing to trigger the activities ofmultiple controller threads that launch</p>
<p class="p1">the application, initiate processing, shut down the system, and create billing records. <span class="s1">GrepTheWeb </span>uses</p>
<p class="p1"><span class="s1">Hadoop MapReduce</span>, an open-source software package that splits a large data set into chunks, distributes</p>
<p class="p1">them acrossmultiple systems, launches the processing, and, when the processing is complete, aggregates</p>
<p class="p1">the outputs from different systems into a final result. <span class="s1">Apache Hadoop </span>is a software library for distributed</p>
<p class="p1">processing of large data sets across clusters of computers using a simple programming model.</p>
<p class="p1">The details of the workflow of <span class="s1">GrepTheWeb </span>are captured in Figure <span class="s2">4.7</span>(b) and consist of the following</p>
<p class="p1">steps [<span class="s2">360</span>]:</p>
<p class="p1"><span class="s1">1. The startup phase. </span>Creates several queues â€“ launch, monitor, billing, and shutdown queues. Starts</p>
<p class="p1">the corresponding controller threads. Each thread periodically polls its input queue and, when a</p>
<p class="p1">message is available, retrieves the message, parses it, and takes the required actions.</p>
<p class="p1"><span class="s1">2. The processing phase. </span>This phase is triggered by a <span class="s3">StartGrep </span>user request; then a launch message</p>
<p class="p1">is enqueued in the launch queue. The launch controller thread picks up the message and executes the</p>
<p class="p1">launch task; then, it updates the status and time stamps in the Amazon <span class="s1">Simple DB </span>domain. Finally,</p>
<p class="p1">it enqueues a message in the monitor queue and deletes the message from the launch queue. The</p>
<p class="p1">processing phase consists of the following steps:</p>
<p class="p1"><span class="s1">a. </span>The launch task starts Amazon <span class="s1">EC2 </span>instances. It uses a Java Runtime Environment preinstalled</p>
<p class="p1">Amazon Machine Image (AMI), deploys required <span class="s1">Hadoop </span>libraries, and starts a <span class="s1">Hadoop </span>Job</p>
<p class="p1">(run <span class="s1">Map/Reduce </span>tasks).</p>
<p class="p1"><span class="s1">b. Hadoop </span>runs map tasks on Amazon <span class="s1">EC2 </span>slave nodes in parallel. A map task takes files from</p>
<p class="p1">Amazon <span class="s1">S3</span>, runs a regular expression, and writes the match results locally, along with a description</p>
<p class="p1">of up to five matches. Then the combine/reduce task combines and sorts the results and</p>
<p class="p1">consolidates the output.</p>
<p class="p1"><span class="s1">c. </span>Final results are stored on Amazon <span class="s1">S3 </span>in the output bucket.</p>
<p class="p1"><span class="s1">3. The monitoring phase. </span>The monitor controller thread retrieves the message left at the beginning of</p>
<p class="p1">the processing phase, validates the status/error in Amazon <span class="s1">Simple DB, </span>and executes themonitor task.</p>
<p class="p1">It updates the status in the Amazon <span class="s1">Simple DB </span>domain and enqueues messages in the shutdown and</p>
<p class="p1">billing queues. The monitor task checks for the <span class="s1">Hadoop </span>status periodically and updates the <span class="s1">Simple</span></p>
<p class="p1"><span class="s1">DB </span>items with status/error and the Amazon <span class="s1">S3 </span>output file. Finally, it deletes the message from the</p>
<p class="p1">monitor queue when the processing is completed.</p>
<p class="p1"><span class="s1">4. The shutdown phase. </span>The shutdown controller thread retrieves the message from the shutdown queue</p>
<p class="p1">and executes the shutdown task, which updates the status and time stamps in the Amazon <span class="s1">Simple DB</span></p>
<p class="p1">domain. Finally, it deletes the message from the shutdown queue after processing. The shutdown</p>
<p class="p1">phase consists of the following steps:</p>
<p class="p1"><span class="s1">a. </span>The shutdown task kills the <span class="s1">Hadoop </span>processes, terminates the <span class="s1">EC2 </span>instances after getting <span class="s1">EC2</span></p>
<p class="p1">topology information from Amazon <span class="s1">Simple DB, </span>and disposes of the infrastructure.</p>
<p class="p1"><span class="s1">b. </span>The billing task gets the <span class="s1">EC2 </span>topology information, <span class="s1">Simple DB </span>usage, and <span class="s1">S3 </span>file and query</p>
<p class="p1">input, calculates the charges, and passes the information to the billing service.</p>
<p class="p1"><span class="s1">5. The cleanup phase. </span>Archives the <span class="s1">Simple DB </span>data with user info.</p>
<p class="p1"><span class="s1">6. User interactions with the system. </span>Get the status and output results. The <span class="s1">GetStatus </span>is applied to the</p>
<p class="p1">service endpoint to get the status of the overall system (all controllers and <span class="s1">Hadoop</span>) and download</p>
<p class="p1">the filtered results from Amazon <span class="s1">S3 </span>after completion.</p>
<p class="p1">To optimize the end-to-end transfer rates in the <span class="s1">S3 </span>storage system, multiple files are bundled up and</p>
<p class="p1">stored as <span class="s1">S3 </span>objects. Another performance optimization is to run a script and sort the keys and the URL</p>
<p class="p1">pointers and upload them in sorted order to <span class="s1">S3</span>. In addition, multiple fetch threads are started in order</p>
<p class="p1">to fetch the objects.</p>
<p class="p1">This application illustrates the means to create an on-demand infrastructure and run it on a massively</p>
<p class="p1">distributed system in a manner that allows it to run in parallel and scale up and down based on the</p>
<p class="p1">number of users and the problem size.</p>
</body>
</html>
